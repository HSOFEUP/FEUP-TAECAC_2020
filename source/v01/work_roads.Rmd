---
title: "accidents"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Outline 
* Load Packages
* Load Data
* Recoding
* Geography Preparation
* Time Series Preparation
* Data Exploration
* Month
* Road Type and Speed Limits
* Day of the Week
* Day of the Week vs Month
* Time Series Exploration
* Plotting Hourly Patterns by Day of Week
* Calculating a Rate
* Numerator: Number Killed or Seriously Injured
* Denominator: Length of Road
* Calculating the Rate
* Mapping KSI per 1000km
* Heatmap of a Single Local Authority
* Ranking Local Authorities by KSI per 1000km

# 1 - Preprocessing

## Load All Packages at Glance
```{r, warning=FALSE, message= FALSE}
require(arules)
require(arulesViz)
require(data.table)

require(dplyr)
require(ggplot2)
require(data.table)
#require(xlsx)
require(readxl)
require(ggmap)
#require(proj4)
require("performanceEstimation")
require("e1071")
require("DMwR") 


```


## Load Data
```{r, warning=FALSE, message= FALSE}
data_dir <- "../data"
df <- read.csv(paste(paste( data_dir, "Accidents_2015.csv", sep= "/" )), stringsAsFactors = FALSE)

# Show some info
(colnames(df))
```


## Show Some Details
```{r, warning=FALSE, message= FALSE}

(str(df))
(ncol(df))
(nrow(df))
```


## Some Preprocessing (Avoid Messy Columns Names)
```{r, warning=FALSE, message= FALSE}

## Change messy column names

setnames( df  , "Local_Authority_.District." , "Local_Authority_District" )
setnames( df  , "Local_Authority_.Highway."  , "Local_Authority_Highway"  )

setnames( df  , "X1st_Road_Class"            , "First_Road_Class"         )
setnames( df  , "X1st_Road_Number"           , "First_Road_Number"        )

setnames( df  , "X2nd_Road_Class"            , "Second_Road_Class"        )
setnames( df  , "X2nd_Road_Number"           , "Second_Road_Number"       )

setnames( df  , "Pedestrian_Crossing.Human_Control"  , "Pedestrian_Crossing-Human_Control"  )
setnames( df  , "Pedestrian_Crossing.Physical_Facilities"  , "Pedestrian_Crossing-Physical_Facilities"  )

(colnames(df))
```
´

## Access Number of Incomplete Cases
```{r, warning=FALSE, message= FALSE, eval=FALSE}

(apply(is.na(df), 2, sum))
```


## Access Number of Levels
```{r, warning=FALSE, message= FALSE, eval=FALSE}

(apply( df, 2, function(x) length( unique(x) )))

# Quick view on data
(head(df))
```


## Encode Some Columns Code Numbers Into Factors (lookup Table)
```{r, warning=FALSE, message= FALSE}

# Some dummy copy 
df_copy <- df 

# Lets map things according lookup table from spreadsheet
map <- data.table( number_sheet  = 3:21,
                   name_variable  = c( 'Police_Force',
                                       'Accident_Severity', 
                                       'Day_of_Week',
                                       'Local_Authority_District', 
                                       'Local_Authority_Highway',
                                       'First_Road_Class', 
                                       'Road_Type',
                                       'Junction_Detail',
                                       'Junction_Control',
                                       'Second_Road_Class', 
                                       'Pedestrian_Crossing-Human_Control',
                                       'Pedestrian_Crossing-Physical_Facilities',
                                       'Light_Conditions',
                                       'Weather_Conditions', 
                                       'Road_Surface_Conditions',
                                       'Special_Conditions_at_Site',
                                       'Carriageway_Hazards',
                                       'Urban_or_Rural_Area', 
                                       'Did_Police_Officer_Attend_Scene_of_Accident')
                   )

## Iterate

for( Index in map$number_sheet ){
  
  look<- data.table(read_excel(paste(data_dir, "Road-Accident-Safety-Data-Guide.xls" , sep= "/" ),
                              sheet = Index))
  
  names(look)[1]            <- tolower( names(look)[1])
  name_variable             <- map[ number_sheet == Index , name_variable ]
  df_copy [[name_variable]] <- look[ match( df[[name_variable]], look[['code']]) ]$label
}


# No need for df and look, lets remove from memory to save space
df <- df_copy

rm(look)
rm(df_copy)

# Quick view on data
(head(df))
```


## Some Features Have Many Factors, Other Dont Contain Any Relevant Info.
* (Indentify those were none/unknow/unclassified, etc are more frequent and take some action if needed.)
```{r, warning=FALSE, message= FALSE}

for ( Index in map$number_sheet ){
  name_variable   <- map[ number_sheet == Index , name_variable ]
  cat ( paste0( "\n\n",name_variable, ":" ))
  print ( table( df[[name_variable]] ))
}

```


# 2 - Exploratory Analysis

## Lets Analyse The Road Type Where Large Number of Vehicles are Involved 
```{r, warning=FALSE, message= FALSE}

th <- 9
# Subset to number of vehicles larger that threshold
df_large_crashes <- df["Number_of_Vehicles" > th, ]
barplot(prop.table(table(df_large_crashes$Road_Type)), las=2, cex.names=.5)
```

*Note: Its kind of obvious that highways and single Carriage account for the majority of cars involved


## Lets Add Speed Limit to the Equation
```{r, warning=FALSE, message= FALSE}

# Adding Spped_limit 
(df_large_crashes %>% group_by(Road_Type, Speed_limit) %>% 
    tally() %>% 
    arrange(desc(n)))
```

*Note: High Speed limits account for majority of cars involved (obvious).
Considering the close proximity, the Road Type (Dual Carriage) and the Speed limit(70), we may find some areas were major crash in series occurs. 
Perhaps some more attention is needed to those spots by local Authorities.


## Lets Bring  the Area Density for a Detailed Analysis
```{r, warning=FALSE, message= FALSE, eval= FALSE}

load(file= "map.Rdata")

UK <- get_map(location = 'England', zoom=8)
map <- ggmap(UK)
```
´

## Some Encoding of (Dates, Day Time, Season, Number Causalities
```{r, warning=FALSE, message= FALSE}

# Conver to datatable for easy encoding
df_copy <-data.table(df)

# Dummy copy
df_copy[, Accident_Index := NULL]


## Convert factor date to date 
df_copy[, Date       := as.Date( as.character( Date ), format = "%d/%m/%Y" ) ]

df_copy[, Day_of_year := as.Date( format( Date, "%m-%d" ), format = "%m-%d" )]

# Now as factors (data as Winter, Spring, Summer, Fall) (To be precise in the days)
df_copy[, Season      := ifelse( Day_of_year >= as.Date( "03-20", format = "%m-%d" ) & Day_of_year < as.Date( "06-21", format = "%m-%d" ) ,"Spring",
                                 ifelse( Day_of_year >= as.Date( "06-21", format = "%m-%d" ) & Day_of_year < as.Date( "09-22", format = "%m-%d" ) ,"Summer",
                                         ifelse( Day_of_year >= as.Date( "09-22", format = "%m-%d" ) & Day_of_year <= as.Date( "12-21", format = "%m-%d" ) ,"Fall", "Winter")))]

# Get the month of the year
df_copy[, c( "Month", "Day_of_year" ) := list( factor( format( Date, "%b" )),NULL ) ]

# Conver hour:minutes only to hours
df_copy[, Hour := as.numeric( substr( Time,1,2 )) ]

# Convert time to Morning, Afternoon, Night and Evening (Reduce Granularity)
breaks <- c( 0, 6, 12, 18, 23 )
labels <- c( "Night", "Morning", "Afternoon", "Evening" )
df_copy[, Day_Period := cut( as.numeric( Hour ), breaks = breaks,
                             labels = labels        , include.lowest=TRUE )]

# Check conversion CHECK
(apply(is.na(df_copy), 2, sum)) # Time 18 not being right converted


# Factorize number casualties ("Small", "Medium", "Large", "Overweelming")
breaks <- c( "0", "2", "4", "12", "inf" )
labels <- c( "Small", "Medium", "Large", "Overweelming" )

df_copy[, "Casualties_Class" := cut( Number_of_Casualties , breaks = breaks    ,
                                   labels = labels      , include.lowest=TRUE )]


# Summarize number of vehicles to (small, medium, high)
breaks <- c( "0", "2", "8",  "inf" )
labels <- c( "Small", "Medium", "Large" )
df_copy [, "Number_Vehicles_Class" := cut( Number_of_Vehicles, breaks = breaks,
                                  labels=labels     , include.lowest=TRUE )]

df_copy$Speed_limit  <- as.factor( df_copy$Speed_limit )


# Local copy to maintain the analisys further in the same domain.
df_subset <- df_copy

```


## Histogram of Accident Severity
```{r, warning=FALSE, message= FALSE}

(table(df_subset$Accident_Severity))
barplot(prop.table(table(df_subset$Accident_Severity)),las=2, cex.names=.5)
```

*Note: Majority of accidents are minor


## Histogram of Number os Causualties per accident
```{r, warning=FALSE, message= FALSE}
barplot(prop.table(table(df_subset$Number_of_Casualties)), cex.names=.5)
```

* Note: Major number casualties are 1 or 2


## Do Some Correlation Beetween Causalities Class and Accident Severity
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Casualties_Class,fill =Accident_Severity)) + 
  geom_bar()+ 
  theme_bw() +
  geom_bar( position = "fill")+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: We observer that when a number of veichles are involved, the larger the number of deaths (highways)


## Show the histogram of the acccidents by day of week
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Day_of_Week)), las=2, cex.names=.5)
```

*Note: On Sundays we see lesst accidents, (people stay at home or drive more relaxed with family).
Also we some increase on fridays, some hurry to reach home, our madness of the start of the weekend..


## Correlated Accident Severety with Day of the Week
```{r, warning=FALSE, message= FALSE}
ggplot( df_subset, aes( Day_of_Week, fill = Accident_Severity ))+ 
  geom_bar()+ 
  theme_bw()+
  geom_bar( position = "fill")+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

* Note: Nothing relevant to observe considering the fatalities group


## Show Commnon Speed Limit at Accident Site
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Speed_limit)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Speed Limit
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Speed_limit,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Interesting, lower speed limits have large count of accidents (local areas)


## Show the road specs were occur more accidents
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Road_Type)), las=2, cex.names=.5)
```


## Correlated Accident severety with Road Type
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Road_Type,fill =Accident_Severity)) + 
  geom_bar()+ 
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+ 
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Single Carriage accouint majority of major accidents and fatal victims


## Show More Common Road Surface Conditions
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Road_Surface_Conditions)), las=2, cex.names=.5)
```


## Correlated Accident severety with Road Surface Conditions
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Road_Surface_Conditions,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+ 
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Most crashed occur on dry or wet conditions, severe dont account victims (people are careful)


## Show the Seasons Accident Histogram 
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Season)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Season
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Season,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Nothing relevant to conclude, summer and fall acocunt more accidents..


## Accident by Month
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Month)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Month
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Month,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+ 
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Slight more accident in july and november


## Accident by Hour
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Hour)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Hour 
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Hour,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Large peak of accidents on the afternoon (people going home after work)


## Summary, by Day Period
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Day_Period)), las=2, cex.names=.5) ## ok obvious, last info tells 17h
```


## Correlated Accident Severety with Day Period
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Day_Period,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Large peak of accidents on the afternoon ( with a visible in fata victims (percentage related))


## Show Where Occur Accidents (Urban, Rural)
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Urban_or_Rural_Area)), las=2, cex.names=.5)
```


## Correlated Accident severety with Urban or Rural
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Urban_or_Rural_Area,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+
  theme_bw()+ 
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Large peak of accidents occur on urban scenarios, whoever theres a slight fatality on rural


## Most Accidents Occurs Daylight, 17H, lets corroborate.
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Light_Conditions)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Light Conditions
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Light_Conditions,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Large peak of accidents occur on urban scenarios, whoever theres a slight fatality on rural


## Show the Fisrt Road Class Number of Accidents
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$First_Road_Class)), las=2, cex.names=.5)
```


## Correlated Accident Severety with First road class
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(First_Road_Class,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Large peak of accidents occur on A type roads, other are severl undetermined


## Most of Accidents Occur in Uncontrolled Juntions. Lets corraborate
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$First_Road_Class)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Junction_Control
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Junction_Control,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+ 
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Large peak of accidents occur when driver must give away priority


## Show the Pedestrial Facilities Where Occur More Accidents
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$`Pedestrian_Crossing-Physical_Facilities`)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Pedestrian_Crossing-Physical_Facilities
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(`Pedestrian_Crossing-Physical_Facilities`,fill =Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+ 
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Most pedestriac accidents occur when no crossing infratusture is nearby!!


## Show the Histogram in Police Force areas called (most active)
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Police_Force)), las=2, cex.names=.5)
```


## Correlated Accident Severety with Police_Force area
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Police_Force, fill = Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+ 
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: metropolitan Police are the most called, obvious, metropolitan have more cars and pedestrians moving!!


## Show the Histogram in Police Force areas called (most active)
```{r, warning=FALSE, message= FALSE}

barplot(prop.table(table(df_subset$Police_Force)), las=2, cex.names=.5)
```


## Show Correlations with Police force area and Weather Conditions  with Severety 
```{r, warning=FALSE, message= FALSE}

ggplot(df_subset, aes(Weather_Conditions, fill = Accident_Severity))+
  geom_bar()+
  theme_bw()+
  geom_bar( position = "fill")+  
  theme_bw()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

*Note: Most accicnet occur on metroploita with no severe weather conditions...


# 3 - Association Rules

## Convert Into Transactions (all must be factors!)

Select Some relevant columns tha may be associated with accident values

*Note: Computing this values takes to long, we load them previous
```{r}
load(file ="df_subset_Ent.Rdata")
load(file ="entropy.gains.Rdata")
load(file ="correlation.filter.selection.Rdata")
```


```{r, warning=FALSE, message= FALSE, eval=FALSE}
library(FSelector)

# Dummy copy
df_rules <- df_model
rm(df_model)

# Create an subset to see the correlation of the variables to give clues about their relevance.
df_rules_ent            <- df_rules[, setdiff( names( df_rules),c( "Longitude",
                                                                 "Latitude",
                                                                 "First_Road_Number",
                                                                 "Second_Road_Number",
                                                                 "Number_of_Casualties" ,
                                                                 "Time",
                                                                 "Date", 
                                                                 "Number_of_Vehicles", 
                                                                 "Hour",
                                                                 "LSOA_of_Accident_Location" ,
                                                                 "Location_Easting_OSGR",
                                                                 "Location_Northing_OSGR" )),
                                  with=F]

# Check the entropy gains regarding the Accident Severity
entropy.gains                  <- information.gain ( Accident_Severity~. , df_rules_ent )

# Compute the corollation
correlation.filter.selection   <- cfs              ( Accident_Severity~. , df_rules_ent )

```


## Show the Variable Selections
```{r, warning=FALSE, message= FALSE, eval= FALSE}

# Select the variables to use
variable.use <- unique( c( cutoff.k( entropy.gains, 11 ), correlation.filter.selection ))

# Show the choices
(variable.use)
```

## Create the subset with the entropy suggestions for laler use.
```{r, eval= FALSE}
## Create the subset with the entropy suggestions for laler use.
subset_entropy <- df_subset[, c( "Accident_Severity", variable.use ), with=F ]
```



## For our Initial Guess, Lets do a Analisys with Variables We Find Relevant.
```{r, warning=FALSE, message= FALSE}
df_rules <- df_subset

# Convert the remainder values to factors
df_rules$Season <- as.factor(df_rules$Season)
df_rules$Hour <- as.factor(df_rules$Hour)

# Here we make our initial selection.
subset <- subset(df_rules, select =c("Accident_Severity",
                                      "Speed_limit",
                                      "Light_Conditions",
                                      "Urban_or_Rural_Area",
                                      "Month",
                                      "Pedestrian_Crossing-Physical_Facilities",
                                      "Day_Period",
                                      "Road_Type",
                                      "Road_Surface_Conditions",
                                      "Junction_Detail",
                                      "Junction_Control",
                                      "Casualties_Class",
                                      "Weather_Conditions",
                                      "Season"))

# Convert then to transacions
trans <- as(subset, "transactions")
summary(trans)

# No longles need subset
rm(subset)
```


## Check Items Labels
```{r, warning=FALSE, message= FALSE}

# Check some items labels
head(itemLabels(trans))
```


## Check Items Frequency
```{r, warning=FALSE, message= FALSE}

# Check the item frequency (top 25)
itemFrequencyPlot(trans, topN=25,  cex.names=.5)
```


## Lest Look at Some Subsets of Transactions Containing the Itemset of Interest
*Note: Focus in the Accident_Severity=Fatal
```{r, warning=FALSE, message= FALSE}

trans_sub <- subset(trans, items %in% "Accident_Severity=Fatal")
itemFrequencyPlot(trans_sub, topN = 25, population = trans, cex.names=.5)
```


## Order Itemset by Lift
```{r, warning=FALSE, message= FALSE}

# Order By lift
itemFrequencyPlot(trans_sub, topN = 25, population = trans, lift=TRUE, cex.names=.5)
```

*Note:  The graph manifest that the Accident_Severity=Fatal appear frequently as the no physical crossing facilities within 50 meters also dry conditions and rural areas.


## Focus in the Light_Conditions=Darkness - lights lit
```{r, warning=FALSE, message= FALSE}

trans_sub <- subset(trans, items %in% "Light_Conditions=Darkness - lights lit")
itemFrequencyPlot(trans_sub, topN = 25, population = trans, cex.names=.5)
```


## Order By lift
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(trans_sub, topN = 25, population = trans, lift=TRUE, cex.names=.5)
```

*Note: The graph manifest that the light conditions - Darkness lights lit Urbarn_or_Rural_Area=Urban and Speed_limit=30 appear frequently, what makes sense to a urban city area. 


## Focus in the Road_Surface_Conditions=Wet or damp
```{r, warning=FALSE, message= FALSE}

trans_sub <- subset(trans, items %in% "Road_Surface_Conditions=Wet or damp")
itemFrequencyPlot(trans_sub, topN = 25, population = trans, cex.names=.5)
```


## Order By lift
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(trans_sub, topN = 25, population = trans, lift=TRUE, cex.names=.5)
```

*Note: The graph manifest that the the Road_Surface_Conditions=Wet_or_damp apper frequently with Road_Type=Single Carriageway. Also ordering by lift we may find a strong co-occurence of Surface_Conditions=Wet or damp with the winter months (Nov, Dec, Jan) what is obvious.


## Focus in the Day_Period=Night
```{r, warning=FALSE, message= FALSE}

trans_sub <- subset(trans, items %in% "Day_Period=Night")
itemFrequencyPlot(trans_sub, topN = 25, population = trans, cex.names=.5)
```


## Order By lift
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(trans_sub, topN = 25, population = trans, lift=TRUE, cex.names=.5)
```

*Note: Analysing the lift graph, we find that the Day_period=Night co-occcur toguether with Light_Conditions=Darkneess - lights unlit and Darkness -no Lighting and we also may find a small co-occurence of the fatal accidents severety. The lift value is not very high.


## Lets Mining Frequent Items
```{r, warning=FALSE, message= FALSE}

# Drop trans_sub so save memory
rm(trans_sub)

# Find an interesting support (have at least 500 transactions)
(500/nrow(trans))
```


## Lets Mining Frequent Items
```{r, warning=FALSE, message= FALSE}

# User parameters
min_supp <- 0.0036
min_conf <- 0.5
min_lift <- 0.6
min_len <- 2
max_len <- 4

## Generate frequent itemsets
itemsets <- apriori(trans, parameter = list(target = "frequent",
  supp=min_supp, minlen = min_len, maxlen =max_len))
```


## Inspect the 10 most Frequent Items
```{r, warning=FALSE, message= FALSE}

inspect(head(sort(itemsets), n=10))
```

*Note: Here we can see that the majority of the accidents severety are Slight and the number of casualties small.

Another info is that the small casuality occur mostly without existence of any pedestrian physiscal facilities.

Another info is that the majority of the small accidents occur in the Single carriage road type.


## Adding a Extra Measure of Quality
```{r, warning=FALSE, message= FALSE}

quality(itemsets)$lift <- interestMeasure(itemsets, measure="lift", trans = trans)

## Lets analyse
inspect(head(sort(itemsets, by = "lift"), n=10))
```


# Plot Itemsets as a Graph. (Different subgroups with Items that are Related to each other can be identified.)
```{r, warning=FALSE, message= FALSE}

plot(head(sort(itemsets, by = "lift"), n=10), method = "graph", control=list(cex=.8))
```

*Note: A first inspection tells that the {Speed_limit=70, Urban_or_Rural_Area=Rural, Road_Type=Dual carriageway, Junction_Detail=Slip road} presents a strong lift (Co-occur frequently).

Also the {Speed_limit=70, Light_Conditions=Darkness - no lighting, Urban_or_Rural_Area=Rural, Road_Type=Dual carriageway} strong lift (Co-occur frequently) .

As we can analyse in the graph, is possible to identify to major groups of itemsets that (Co-occur frequently), namelly the darkness condition and rural areas

Also is possible to identify the (Co-occur frequently) of Surface conditions Dry, and not and junction within 20 meters.


## Lets Refine our Frequent Itemsets Generations combining other more refined Items of interest.
```{r, warning=FALSE, message= FALSE}

# Generate the subset transactions containing only Fatal Accidents
trans_sub <- subset(trans, items %in% "Accident_Severity=Fatal")

# Generate the itemsets
itemsets_subset <- apriori(trans_sub, parameter = list(target = "frequent",
  supp=min_supp, minlen = min_len, maxlen=max_len))
```


## Inspect the 10 most frequent item
```{r, warning=FALSE, message= FALSE}

inspect(head(sort(itemsets_subset), n=10))
```

## Adding a Extra Measure of Quality
```{r, warning=FALSE, message= FALSE}

quality(itemsets_subset)$lift <- interestMeasure(itemsets_subset,
                                          measure="lift", trans = trans_sub)

# Lets analyse
inspect(head(sort(itemsets_subset, by = "lift"), n=10))
```


# Lets plot
```{r, warning=FALSE, message= FALSE}

plot(head(sort(itemsets_subset, by = "lift"), n=10), method = "graph", control=list(cex=.8))
```

*Note: A quick analisys shows that Accident_Severity=Fatal, Light_Conditions=Darkness - lights lit, Road_Type=Roundabout, Junction_Detail=Roundabout have an reasonable lift suggesting that they Co-occurr together, what makes sense regarding the fact that suggest a city crossing, (City))


## Subset the Overwellming class
```{r, warning=FALSE, message= FALSE}

# Generate the subset transactions containing only overwellming number casualties
trans_sub <- subset(trans, items %in% "Casualties_Class=Overweelming")

# Generate the itemsets
itemsets_subset <- apriori(trans_sub, parameter = list(target = "frequent",
                                                       supp=min_supp, minlen = min_len, maxlen=max_len))
```


# Inspect the 10 most frequent item
```{r, warning=FALSE, message= FALSE}

inspect(head(sort(itemsets_subset), n=10))
```


## Attain only those with large number of casualties involved
```{r, warning=FALSE, message= FALSE}

itemsets_subset <- subset(itemsets_subset, subset = items %in% "Casualties_Class=Overweelming")

quality(itemsets_subset)$lift <- interestMeasure(itemsets_subset,
                                                 measure="lift", trans = trans_sub)

# Lets analyse
inspect(head(sort(itemsets_subset, by = "lift"), n=10))
```


## Lets plot
```{r, warning=FALSE, message= FALSE}

plot(head(sort(itemsets_subset, by = "lift"), n=10), method = "graph", control=list(cex=.8))
```

*Note:  {Accident_Severity=Fatal, Month=feb., Casualties_Class=Overweelming,Weather_Conditions=Fog or mist} We may find that fog, february, Fatal accidnet and number of casuaties high co-occur frequently. An typical conditions to have major accidents


## Mine Association Rules
```{r, warning=FALSE, message= FALSE}

rules <- apriori(trans, parameter = list(supp=min_supp, maxlen=max_len, target="rules"))
```


## Quick inspection
```{r, warning=FALSE, message= FALSE}

inspect(head(sort(rules, by="lift"), n=10))
```


## Plot the scatterplot
```{r, warning=FALSE, message= FALSE}

plot(rules)
```


## Lower the support value
```{r, warning=FALSE, message= FALSE}

(200/nrow(trans))

min_supp = 0.0015

# Generate the items
rules <- apriori(trans, parameter = list(supp=min_supp, maxlen=max_len, target ="rules"))
```


## Quick inspection
```{r, warning=FALSE, message= FALSE}

inspect(head(sort(rules, by="lift"), n=10))
```

## Plot the scatterplot
```{r, warning=FALSE, message= FALSE}

plot(rules)
```

*Note: We may find some rules that area obvious to us such as Speed_limit=60, Month=Nov, Day_Period=Evening} => {Light_Conditions=Darkness - no lighting}. In order to obtain more relevant ones we gona focus in the cases that acciddents are fatal.


## Subset the Accident_Severity=Fatal
```{r, warning=FALSE, message= FALSE}

r_subset <- subset(rules, subset = items %in% "Accident_Severity=Fatal")

## Inspect
inspect(head(sort(r_subset, by="lift"), 10))
```


## Lets plot
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(items(r_subset), topN=10, cex.names=.5)
```

## Plot graph by lift
```{r, warning=FALSE, message= FALSE}

plot(head(sort(r_subset, by="lift"), 10),
  method="graph", control=list(cex=.7))
```

*Note: We may find that {Accident_Severity=Fatal, Speed_limit=60, Light_Conditions=Daylight} => {Urban_or_Rural_Area=Rural} Co-occur together the lift is not very high, but is bigger than one what may suggest that they are positive co-related.


## Subset by Light conditions to find some Patterns
```{r, warning=FALSE, message= FALSE}

r_subset <- subset(rules, subset = items %in% "Light_Conditions=Darkness - no lighting")

## Inspect
inspect(head(sort(r_subset, by="lift"), n=10))
```


## Plot itemfrequency
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(items(r_subset), topN=10, cex.names=0.5)
```


## Show graph
```{r, warning=FALSE, message= FALSE}

plot(head(sort(r_subset, by="lift"), 10),
  method="graph", control=list(cex=.7))
```

*Note: From an quick analisys, Speed_limit=60, Month=Nov, Day_Period=Evening} => {Light_Conditions=Darkness - no lighting} is a coerent rule but not very usefull.


##Subset by Road Surface conditions
```{r, warning=FALSE, message= FALSE}

r_subset <- subset(rules, subset = items %in% "Road_Surface_Conditions=Wet or damp")

# Inspect
inspect(head(sort(r_subset, by="lift"), n=10))
```


## Plot itemfrequency
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(items(r_subset), topN=10, cex.names=0.5) #(Too big to plot)
```


## Show graph
```{r, warning=FALSE, message= FALSE}

plot(head(sort(r_subset, by="lift"), 10),
  method="graph", control=list(cex=.7))
```

*Note: We may find that Light_Conditions=Darkness - no lighting, Road_Type=Dual carriageway, Road_Surface_Conditions=Wet or damp} => {Speed_limit=70} have an lift value > 1 suggesting that they co-occur together (of course wet conditions and Darkness leads to more accidents).


## Subset by road type.
```{r, warning=FALSE, message= FALSE}

r_subset <- subset(rules, subset = items %in% "Road_Type=Dual carriageway")

# Inspect
inspect(head(sort(r_subset, by="lift"), n=10))
```

## Plot itemfrequency
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(items(r_subset), topN=10,  cex.names=0.5) #(Too big to plot)
```

## Show graph
```{r, warning=FALSE, message= FALSE}

plot(head(sort(r_subset, by="lift"), 10),
  method="graph", control=list(cex=.7))
```

*Note: We may find the rule {Light_Conditions=Darkness - no lighting, Road_Type=Dual carriageway, Casualties_Class=Medium} => {Speed_limit=70} contains a co-occurence of high number of car involved. Clearely a auto -way.


## From our generated rules lets find out those with more conviction (High Values of conviction (indicates that the consequent depends strongly in the antecendent)
```{r, warning=FALSE, message= FALSE}

rules_convi <- cbind(as(rules, "data.frame"),
                              conviction=interestMeasure(rules,
                                                         "conviction", trans))
# Order by descreasing value of conviction
rules_convi <- rules_convi[order(rules_convi$conviction),]

# Get only the top ones%
per <-0.0005 # define percentage
top_rules <- head(rules_convi[order(rules_convi$conviction, decreasing = T),], per*nrow(rules_convi))

# Show then (they are ordered by conviction)
(top_rules$rules)
```

*Note: Of course the most commun ones appears first with strong conviction. However we are insterested in a more grain set of groups of itemsets and rules, namelly those that contain Fatal accidents as consequent.


## Try to find rules that have in the consequent Accident_Severity=Fatal
```{r, warning=FALSE, message= FALSE}

# From our rules, subset then
rules_subset <- subset(rules, subset=(rhs %in% c("Accident_Severity=Fatal")))

# Check if any rules were generated
summary(rules_subset)
```

*Note: The current parameters values were not able to generate the items that correspond to the fatal accidents as consequent. (Rare Cases)


## Lets lower our standards trying to catch the fatal ones
```{r, warning=FALSE, message= FALSE}

# In order to save our memory we gonna subset the transactions that contain fatal ones
trans_sub <- subset(trans, items %in% "Accident_Severity=Fatal")

## Set new parameters
(1/nrow(trans_sub))
```


```{r, warning=FALSE, message= FALSE}
min_supp <- 50/nrow(trans_sub)
min_conf <- 0.00025

# Generate the rules with consequent as Accident_Severity=Fatal"
rules_subset <- apriori(trans_sub, parameter = list(supp=min_supp, conf=min_conf,
                                                target ="rules"),
                        appearance = list(rhs=c("Accident_Severity=Fatal")))
```


## Check if any rules were generated
```{r, warning=FALSE, message= FALSE}

summary(rules_subset)
```


## Ok some rules where generated,  inspect then
```{r, warning=FALSE, message= FALSE}

inspect(head(sort(subset(rules_subset, subset= rhs %pin% "Accident_Severity=Fatal")), by="lift"), n=5)
```


## Plot itemfrequency
```{r, warning=FALSE, message= FALSE}

itemFrequencyPlot(items(rules_subset), topN=10, cex.names=0.5) #(Too big to plot)
```


## Show graph
```{r, warning=FALSE, message= FALSE}

plot(head(sort(rules_subset, by="lift"), 10),
  method="graph", control=list(cex=.7))
```

*Note: The diagram may suggest that the fatal accidents happens with no particular positive correlations (lift =1 -> Independent).


## Lest find out in witch conditions accurs major accidents with many cars involved
```{r, warning=FALSE, message= FALSE}

# In order to save our memory we gonna subset the transactions that Casualties_Class=Large
trans_sub <- subset(trans, items %in% "Casualties_Class=Large")

# Set new parameters
(500/nrow(trans_sub))
```


```{r, warning=FALSE, message= FALSE}

min_supp <- 500/nrow(trans_sub)
min_conf <- 0.00025

# Generate the rules with consequent as Accident_Severity=Fatal"
rules_subset <- apriori(trans_sub, parameter = list(supp=min_supp, conf=min_conf,
                                                    target ="rules"),
                        appearance = list(rhs=c("Casualties_Class=Large")))

```


## Check if any rules were generated
```{r, warning=FALSE, message= FALSE}

summary(rules_subset)
```


## Ok some rules where generated,  inspect then
```{r, warning=FALSE, message= FALSE}

inspect(head(sort(subset(rules_subset, subset= rhs %pin% "Casualties_Class=Large")), by="lift"), n=5)
```

*Note: Clearely an rule regarding a high speed motorway


## For The subset obtained with the entropy method, lets do a fine analisys
```{r, warning=FALSE, message= FALSE, eval= FALSE}

# Quick copy to enable reuse of code
subset <- subset_entropy

# Remove so save memory
rm(subset_entropy)

# Convert to transactions
trans <- as(subset, "transactions")
summary(trans)
```

```{r, warning=FALSE, message= FALSE , eval= FALSE}

(500/nrow(trans))
```


```{r, warning=FALSE, message= FALSE, eval= FALSE}

min_supp <- 0.0036
min_conf <- 0.5
min_lift <- 0.6
min_len <- 2
max_len <- 4

# Generate frequent itemsets
itemsets <- apriori(trans, parameter = list(target = "frequent",
                                            supp=min_supp, minlen = min_len, maxlen =max_len))
```


## Inspect the 10 most frequent item
```{r, warning=FALSE, message= FALSE, eval= FALSE}

inspect(head(sort(itemsets), n=10))
```


## Adding a extra measure of quality
```{r, warning=FALSE, message= FALSE, eval= FALSE}

quality(itemsets)$lift <- interestMeasure(itemsets, measure="lift", trans = trans)

# Lets analyse
inspect(head(sort(itemsets, by = "lift"), n=10))
```


```{r, warning=FALSE, message= FALSE, eval= FALSE}
trans_sub <- subset(trans, items %in% "Accident_Severity=Fatal")

## Generate the itemsets
itemsets_subset <- apriori(trans_sub, parameter = list(target = "frequent",
                                                       supp=min_supp, minlen = min_len, maxlen=max_len))
```

## Inspect the 10 most frequent item
```{r, warning=FALSE, message= FALSE, eval= FALSE}

inspect(head(sort(itemsets_subset), n=10))
```

*Note: Police always attend fatal accidents, norma


## Generate rules with rhs as fatal
Generate the rules with consequent as Accident_Severity=Fatal"
```{r, warning=FALSE, message= FALSE, eval= FALSE}

rules_subset <- apriori(trans_sub, parameter = list(supp=min_supp, conf=min_conf,
                                                    target ="rules"),
                        appearance = list(rhs=c("Accident_Severity=Fatal")))
```


## Check if any rules were generated
```{r, warning=FALSE, message= FALSE, eval= FALSE}

summary(rules_subset)
```


## Ok some rules where generated,  inspect then
```{r, warning=FALSE, message= FALSE, eval=FALSE}

inspect(head(sort(subset(rules_subset, subset= rhs %pin% "Accident_Severity=Fatal")), by="lift"), n=5)
```

*Note: We see that the almost the same results were attained.
Police always attend fata crashes
{Junction_Detail=Not at junction or within 20 metres} => {Accident_Severity=Fatal} Ok no infraestructure or juctions.

```{r}


```


# 3 - Modeling

## Letf firt handle the incmolete cases and infer of just simply remove
```{r, warning=FALSE, message= FALSE}

# Staring with our last dataframe preprocessed
df_model <- df_subset # make shallow copy 

# Show number inconplete cases
(apply(is.na(df_model), 2, sum))

# Show number of entries
(nrow(df_model))

# Drop datasframes
#rm(df_subset)
#rm(df_copy)


```

*Note: Second_Road_Class has to many incomplete cases, just remove it, and locations esting and lsoa accident

## Removing Column Second Road Class and Remove Other Incomplete Cases
```{r, warning=FALSE, message= FALSE}


# Lets drop initialy the time to see things, 
df_model <- df_model[, -c('Date', 'Time', 'Location_Easting_OSGR', 'Location_Northing_OSGR',
                              'LSOA_of_Accident_Location','Second_Road_Class', 'First_Road_Number', 'Second_Road_Number', 'Accident_Severity', 'Day_of_Week')]

# We have enough examples for modeling, just remove incomplete entries (only few of then)
df_model <- na.omit(df_model) 

# Show number incomplete cases
(apply(is.na(df_model), 2, sum))

# Show number of entries
(nrow(df_model))

# Show number of levels each column (Levels columns only)
(apply( df_model, 2, function(x) length( unique(x) )))

# Quick view on data
(head(df_model))

# Quick vies on column type see if all its ok
(str(df_model))

```

*Note: perhaps latitude and longitus is to descriminative, lest drop it for now and find a way to summarize this later

## Drop Latitude and logitude
```{r, warning=FALSE, message= FALSE}

# Lets drop initialy the time to see things
df_model <- df_model[, -c('Latitude', 'Longitude')]

# Show number of levels each column (Levels columns only)
(apply( df_model, 2, function(x) length( unique(x) )))

# Debug
(nrow(df_model))
```


## Considering the fact that we want to predict the number of fatalities given features lets model an ML to capture this
#### Lest split out testset (stratifyed?)
```{r, warning=FALSE, message= FALSE}


# Convert chars to factors in this reduced space
#df_model <- mutate_if(df_model, is.character, as.factor)

# Wuick pick
(nrow(df_model))

# Show number incomplete cases
(apply(is.na(df_model), 2, sum))

#rm(df_copy)
rm(map)

# convert to upper case coliumns, some models dont handle this well
for( i in colnames(df_model)){
    colnames(df_model)[which(colnames(df_model)==i)] = toupper(i)
 }


# Ok, there is here a column messi, lest remove the -
setnames( df_model  , "PEDESTRIAN_CROSSING-HUMAN_CONTROL"  , "PEDESTRIAN_CROSSING_HUMAN_CONTROL"  )
setnames( df_model  , "PEDESTRIAN_CROSSING-PHYSICAL_FACILITIES"  , "PEDESTRIAN_CROSSING_PHYSICAL_FACILITIES")


# Ensure thet they are factors
library(dplyr)
df_model=df_model %>% mutate_if(is.character, as.factor)

# This column has 53 categoriees, lest drop them
df_model <- df_model[, -c('LOCAL_AUTHORITY_DISTRICT')]

#Create an small sample (2000 cases)
sample_perc <- 0.05

# Extract samples for hold out method
set.seed(1234) # for reproduction
idx_sample <-sample(1:nrow(df_model),as.integer(sample_perc*nrow(df_model)))

length(idx_sample)

df_model <- df_model[idx_sample, ]


# Set the threshold
split_val <- 0.7

# Extract samples for hold out method
set.seed(1234) # for reproduction
idx.tr <-sample(1:nrow(df_model),as.integer(split_val*nrow(df_model)))
tr <- df_model[idx.tr,]
ts <- df_model[-idx.tr,] 

ts = rbind(tr[1,],ts)
ts = ts[-1,]

# We should ensure a stratifyed sample
#library(caret)

#idx.tr <- createDataPartition(df_model, p = split_val, list = FALSE)
#tr <- df_model[idx.tr,]
#ts <- df_model[-idx.tr,] 

# Debug
(str(tr))
(nrow(tr))
(nrow(ts))

```


## Setup an Simple baseline using trees (considering the size of dataset, gonna use holdout)
```{r, warning=FALSE, message= FALSE, eval=FALSE}
  
# Make several estimations and select the best model
res_TREES <- performanceEstimation(
  PredTask(NUMBER_OF_CASUALTIES~ .,tr),
  workflowVariants(learner="rpartXse",learner.pars=list(se=c(0,0.5,1, 2, 3))),
  EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))

# Save the model results
save(res_TREES, file = "res_TREES1.RData")

```

## Load TREES
```{r, warning=FALSE, message= FALSE}

# Load the data
load(file = "res_TREES1.RData")

# Plot the graphs
plot(res_TREES)

# See the performance rank
topPerformers(res_TREES)

# Get the parameters
(getWorkflow("rpartXse.v2",res_TREES))

```

* Note: We see that increasing the number of se leads to wprse results



## Setup an RF Model (here im preloding models avoid training time)
```{r, warning=FALSE, message= FALSE, eval=FALSE}

require('randomForest')

#df_model1 <- df_model[, -c('LOCAL_AUTHORITY_DISTRICT')]

# Make several estimations and select the best model
res_RF <- performanceEstimation(
  PredTask(NUMBER_OF_CASUALTIES ~ .,tr),
  workflowVariants(learner="randomForest", learner.pars=list(ntree= c(100, 500, 1000))), 
 EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))

# Save the model results
save(res_RF, file = "res_RF.RData")

```


## Load RF
```{r, warning=FALSE, message= FALSE}

# Load the data
load(file = "res_RF.RData")

# Plot the graphs
plot(res_RF)

# See the performance rank
topPerformers(res_RF)

```


## Setup an SVM Model
```{r, warning=FALSE, message= FALSE, eval=FALSE}

# Make several estimations and select the best model
res_SVM <- performanceEstimation(
  PredTask(NUMBER_OF_CASUALTIES ~ .,tr),
  workflowVariants(learner="svm", learner.pars=list(kernel=c("linear", "radial"), cost=c(1,3), gamma=c(0.1,0.01), epsilon=0.1)),

 EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))

# Save the model results
save(res_SVM, file = "res_SVM.RData")

```

## Load SVM
```{r, warning=FALSE, message= FALSE}

# Load the data
load(file = "res_SVM.RData")

# Plot the graphs
plot(res_SVM)

# See the performance rank
topPerformers(res_SVM)

```


## Setup Several models
```{r, warning=FALSE, message= FALSE, eval= FALSE}

# Make several estimations and select the best model
res_EXP <- performanceEstimation(
   PredTask(NUMBER_OF_CASUALTIES ~ ., tr),
   c( Workflow(learner="naiveBayes"),
        workflowVariants(learner="svm", learner.pars=list(kernel=c("linear", "radial"), cost=c(1,3))),
      Workflow(learner="randomForest", learner.pars=list( ntree=3000))
        ),
   EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))

# Save the model results
save(res_EXP, file = "res_EXP.RData")

```

## Load EXP
```{r, warning=FALSE, message= FALSE}

# Load the data
load(file = "res_EXP.RData")

# Plot the graphs
plot(res_EXP)

# See the performance rank
topPerformers(res_EXP)

```


## Set a buch of modle with default parameters
```{r, warning=FALSE, message= FALSE, eval= FALSE}

res_MODELS <- performanceEstimation(
  PredTask(NUMBER_OF_CASUALTIES ~ ., tr),
  workflowVariants("standardWF",
                   learner=c("rpartXse","svm","randomForest")),
  EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))

# Save the model results
save(res_MODELS, file = "res_MODELS.RData")

```

```{r, warning=FALSE, message= FALSE}

# Load the data
load(file = "res_MODELS.RData")

# Plot the graphs
plot(res_MODELS)

# See the performance rank
topPerformers(res_MODELS)

```


# 4 - Clustering

## Lets try some Clustering and Analyse (first we gonna start by dropping some locations and time columns, and some incomplete columns) Whe are interested in predict the number of casualties, so we drop the target variable alsp
```{r, warning=FALSE, message= FALSE}

require('cluster')
require(cluster)
require(dplyr)
require(ggplot2)
require(readr)
require(Rtsne)

# Some intial colum name analisis
(colnames(df_subset))

# Lets drop initialy the time to see things
df_notime <- df_subset[, -c('Date', 'Time', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'LSOA_of_Accident_Location','Second_Road_Class', 'Number_of_Casualties')]

# Access the number of levels
(apply(df_notime, 2, function(x) length( unique(x) )))

# Access the incomplete cases
(apply(is.na(df_notime), 2, sum))

# Count incomplete cases
sum(apply(df_notime, 1, function(x){any(is.na(x))}))
 
# Remove those incomplete cases in small number
df_notime <- na.omit(df_notime)

```


## Lets just subset our dataset for memory issues
```{r, warning=FALSE, message= FALSE}
# Ok, we have memory problems, lest just pick a small subset of rows
split_val <- 0.05

# Extract samples for hold out method
set.seed(1234) # for reproduction
idx.tr <-sample(1:nrow(df_notime),as.integer(split_val*nrow(df_notime)))
df_notime_sample <- df_notime[idx.tr,]

# Convert chars to factors in this reduced space
df_notime_sample <- mutate_if(df_notime_sample, is.character, as.factor)

# Wuick pick
(nrow(df_notime_sample))

```


## Explore Gower distance, enabling to computed as the average of partial dissimilarities across individuals.
```{r}

# Compute Gower distance
gower_dist <- daisy(df_notime_sample, metric = "gower")
gower_mat <- as.matrix(gower_dist)

#' Print most similar clients
(df_notime_sample[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]), arr.ind = TRUE)[1, ], ])

#' Print most dissimilar clients
(df_notime_sample[which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]), arr.ind = TRUE)[1, ], ])

```


## Partial dissimilarities (d_ij^f) computation depend on the type of variable being evaluated.
The PAM algorithm searches for the k representative objects (the
medoids) among the cases in the given data set.

As with k-means each observation is allocated to the nearest
medoid.

PAM is more robust to the presence of outliers because it uses
original objects as centroids instead of averages that may be
subject to the effects of outliers

```{r}
sil_width <- c(NA)
for(i in 2:8){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}

plot(1:8, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:8, sil_width)

```

* Note: Acccording to the elbow technique, the optimal number of cluster is 5


## Lets form the clusters with 6 distictivive classes
```{r, warning=FALSE, message= FALSE}
k <- 5
pam_fit <- pam(gower_dist, diss = TRUE, k)
pam_results <- df_notime_sample %>%
  mutate(cluster = pam_fit$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results$the_summary

```


## Visualization of the cluster in a lower dimensional space
```{r, warning=FALSE, message= FALSE}

tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering))
ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))

```

* Note: Is possoble to observe a dense red area, confirming the relevancy of the segmentation.


## Lets try some Clustering and Analyse over time (trimester by trimester)
```{r, warning=FALSE, message= FALSE, eval= FALSE}

# Create the array list to iterate
# Considering that we are subsetting, we can handle the subset all. condideting dates, lets make motn by mothn

# Lets set out array to iterate
month_array <- c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')

# Iterate and do things
for ( month in month_array){ 
  
  print(month)
  
  # Some intial colum name analisis
  (colnames(df_subset))
  
  # Lets drop initialy the time to see things
  df_notime_temp <- df_subset[, -c('Date', 'Time', 'Location_Easting_OSGR', 'Location_Northing_OSGR',
                              'LSOA_of_Accident_Location','Second_Road_Class', 'Number_of_Casualties')]

  # Access the number of levels
  (apply(df_notime_temp, 2, function(x) length( unique(x) )))

  # Access the incomplete cases
  (apply(is.na(df_notime_temp), 2, sum))

  # Count incomplete cases
  sum(apply(df_notime_temp, 1, function(x){any(is.na(x))}))
 
  # Remove those incomplete cases in small number
  df_notime_temp <- na.omit(df_notime_temp)
  
  # Convert chars to factors in this reduced space
  df_notime_temp <- mutate_if(df_notime_temp, is.character, as.factor)

  # Compute Gower distance
  gower_dist <- daisy(df_notime_temp, metric = "gower")
  gower_mat <- as.matrix(gower_dist)
  
  # Do pam clustering Medoid
  k <- 5
  pam_fit <- pam(gower_dist, diss = TRUE, k)
  pam_results <- df_notime_sample %>%
    mutate(cluster = pam_fit$clustering) %>%
    group_by(cluster) %>%
    do(the_summary = summary(.))
  pam_results$the_summary
  
  # Do some representation
  tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
  tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
    setNames(c("X", "Y")) %>%
    mutate(cluster = factor(pam_fit$clustering))

  ggplot(aes(x = X, y = Y), data = tsne_data) +
    geom_point(aes(color = cluster))
  
  
}

```

