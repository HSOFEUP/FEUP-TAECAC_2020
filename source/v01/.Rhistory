version
install.packages(ggplot)
"ggplot2"
install.packages("ggplot2")
install.packages("e1023")
install.packages("e1071")
install.packages("performanceEstimation")
install.packages("DMwR")
install.packages("randomForest")
install.packages("MASS")
install.packages("dplyr")
install.packages(care\)
install.packages("caret")
install.packages(c("earth", "gbm", "ggvis", "knitr", "readxl"))
setwd("/Volumes/shared/HSO_CLOUD/Dropbox/HSO_FEUP/PRODEI/TAECAC/project/tp1/source")
require(arules)
require(arulesViz)
require(data.table)
require(dplyr)
require(ggplot2)
require(data.table)
#require(xlsx)
require(readxl)
require(ggmap)
#require(proj4)
require("performanceEstimation")
require("e1071")
require("DMwR")
data_dir <- "../data"
df <- read.csv(paste(paste( data_dir, "Accidents_2015.csv", sep= "/" )), stringsAsFactors = FALSE)
# Show some info
(colnames(df))
setnames( df  , "Local_Authority_.District." , "Local_Authority_District" )
setnames( df  , "Local_Authority_.Highway."  , "Local_Authority_Highway"  )
setnames( df  , "X1st_Road_Class"            , "First_Road_Class"         )
setnames( df  , "X1st_Road_Number"           , "First_Road_Number"        )
setnames( df  , "X2nd_Road_Class"            , "Second_Road_Class"        )
setnames( df  , "X2nd_Road_Number"           , "Second_Road_Number"       )
setnames( df  , "Pedestrian_Crossing.Human_Control"  , "Pedestrian_Crossing-Human_Control"  )
setnames( df  , "Pedestrian_Crossing.Physical_Facilities"  , "Pedestrian_Crossing-Physical_Facilities"  )
(colnames(df))
df_copy <- df
# Lets map things according lookup table from spreadsheet
map <- data.table( number_sheet  = 3:21,
name_variable  = c( 'Police_Force',
'Accident_Severity',
'Day_of_Week',
'Local_Authority_District',
'Local_Authority_Highway',
'First_Road_Class',
'Road_Type',
'Junction_Detail',
'Junction_Control',
'Second_Road_Class',
'Pedestrian_Crossing-Human_Control',
'Pedestrian_Crossing-Physical_Facilities',
'Light_Conditions',
'Weather_Conditions',
'Road_Surface_Conditions',
'Special_Conditions_at_Site',
'Carriageway_Hazards',
'Urban_or_Rural_Area',
'Did_Police_Officer_Attend_Scene_of_Accident')
)
## Iterate
for( Index in map$number_sheet ){
look<- data.table(read_excel(paste(data_dir, "Road-Accident-Safety-Data-Guide.xls" , sep= "/" ),
sheet = Index))
names(look)[1]            <- tolower( names(look)[1])
name_variable             <- map[ number_sheet == Index , name_variable ]
df_copy [[name_variable]] <- look[ match( df[[name_variable]], look[['code']]) ]$label
}
# No need for df and look, lets remove from memory to save space
df <- df_copy
rm(look)
rm(df_copy)
# Quick view on data
(head(df))
# Conver to datatable for easy encoding
df_copy <-data.table(df)
# Dummy copy
df_copy[, Accident_Index := NULL]
## Convert factor date to date
df_copy[, Date       := as.Date( as.character( Date ), format = "%d/%m/%Y" ) ]
df_copy[, Day_of_year := as.Date( format( Date, "%m-%d" ), format = "%m-%d" )]
# Now as factors (data as Winter, Spring, Summer, Fall) (To be precise in the days)
df_copy[, Season      := ifelse( Day_of_year >= as.Date( "03-20", format = "%m-%d" ) & Day_of_year < as.Date( "06-21", format = "%m-%d" ) ,"Spring",
ifelse( Day_of_year >= as.Date( "06-21", format = "%m-%d" ) & Day_of_year < as.Date( "09-22", format = "%m-%d" ) ,"Summer",
ifelse( Day_of_year >= as.Date( "09-22", format = "%m-%d" ) & Day_of_year <= as.Date( "12-21", format = "%m-%d" ) ,"Fall", "Winter")))]
# Get the month of the year
df_copy[, c( "Month", "Day_of_year" ) := list( factor( format( Date, "%b" )),NULL ) ]
# Conver hour:minutes only to hours
df_copy[, Hour := as.numeric( substr( Time,1,2 )) ]
# Convert time to Morning, Afternoon, Night and Evening (Reduce Granularity)
breaks <- c( 0, 6, 12, 18, 23 )
labels <- c( "Night", "Morning", "Afternoon", "Evening" )
df_copy[, Day_Period := cut( as.numeric( Hour ), breaks = breaks,
labels = labels        , include.lowest=TRUE )]
# Check conversion CHECK
(apply(is.na(df_copy), 2, sum)) # Time 18 not being right converted
# Factorize number casualties ("Small", "Medium", "Large", "Overweelming")
breaks <- c( "0", "2", "4", "12", "inf" )
labels <- c( "Small", "Medium", "Large", "Overweelming" )
df_copy[, "Casualties_Class" := cut( Number_of_Casualties , breaks = breaks    ,
labels = labels      , include.lowest=TRUE )]
# Summarize number of vehicles to (small, medium, high)
breaks <- c( "0", "2", "8",  "inf" )
labels <- c( "Small", "Medium", "Large" )
df_copy [, "Number_Vehicles_Class" := cut( Number_of_Vehicles, breaks = breaks,
labels=labels     , include.lowest=TRUE )]
df_copy$Speed_limit  <- as.factor( df_copy$Speed_limit )
# Local copy to maintain the analisys further in the same domain.
df_subset <- df_copy
# Staring with our last dataframe preprocessed
df_model <- df_subset # make shallow copy
# Show number inconplete cases
(apply(is.na(df_model), 2, sum))
# Show number of entries
(nrow(df_model))
# Drop datasframes
#rm(df_subset)
#rm(df_copy)
# Lets drop initialy the time to see things,
df_model <- df_model[, -c('Date', 'Time', 'Location_Easting_OSGR', 'Location_Northing_OSGR',
'LSOA_of_Accident_Location','Second_Road_Class', 'First_Road_Number', 'Second_Road_Number', 'Accident_Severity', 'Day_of_Week')]
# We have enough examples for modeling, just remove incomplete entries (only few of then)
df_model <- na.omit(df_model)
# Show number incomplete cases
(apply(is.na(df_model), 2, sum))
# Show number of entries
(nrow(df_model))
# Show number of levels each column (Levels columns only)
(apply( df_model, 2, function(x) length( unique(x) )))
# Quick view on data
(head(df_model))
# Quick vies on column type see if all its ok
(str(df_model))
# Lets drop initialy the time to see things
df_model <- df_model[, -c('Latitude', 'Longitude')]
# Show number of levels each column (Levels columns only)
(apply( df_model, 2, function(x) length( unique(x) )))
# Debug
(nrow(df_model))
# Convert chars to factors in this reduced space
#df_model <- mutate_if(df_model, is.character, as.factor)
# Wuick pick
(nrow(df_model))
# Show number incomplete cases
(apply(is.na(df_model), 2, sum))
#rm(df_copy)
rm(map)
# convert to upper case coliumns, some models dont handle this well
for( i in colnames(df_model)){
colnames(df_model)[which(colnames(df_model)==i)] = toupper(i)
}
# Ok, there is here a column messi, lest remove the -
setnames( df_model  , "PEDESTRIAN_CROSSING-HUMAN_CONTROL"  , "PEDESTRIAN_CROSSING_HUMAN_CONTROL"  )
setnames( df_model  , "PEDESTRIAN_CROSSING-PHYSICAL_FACILITIES"  , "PEDESTRIAN_CROSSING_PHYSICAL_FACILITIES")
# Ensure thet they are factors
library(dplyr)
df_model=df_model %>% mutate_if(is.character, as.factor)
# This column has 53 categoriees, lest drop them
df_model <- df_model[, -c('LOCAL_AUTHORITY_DISTRICT')]
sample_perc <- 0.2
# Extract samples for hold out method
set.seed(1234) # for reproduction
idx_sample <-sample(1:nrow(df_model),as.integer(sample_perc*nrow(df_model)))
nrow(idx_sample)
idx_sample
sample_perc <- 0.3
# Extract samples for hold out method
set.seed(1234) # for reproduction
idx_sample <-sample(1:nrow(df_model),as.integer(sample_perc*nrow(df_model)))
idx_sample
length(idx_sample)
#Create an small sample (2000 cases)
sample_perc <- 0.1
# Extract samples for hold out method
set.seed(1234) # for reproduction
idx_sample <-sample(1:nrow(df_model),as.integer(sample_perc*nrow(df_model)))
length(idx_sample)
#Create an small sample (2000 cases)
sample_perc <- 0.05
# Extract samples for hold out method
set.seed(1234) # for reproduction
idx_sample <-sample(1:nrow(df_model),as.integer(sample_perc*nrow(df_model)))
length(idx_sample)
df_model <- df_model[, idx_sample]
df_model <- df_model[idx_sample, ]
nrow(df_model)
# Set the threshold
split_val <- 0.7
# Extract samples for hold out method
set.seed(1234) # for reproduction
idx.tr <-sample(1:nrow(df_model),as.integer(split_val*nrow(df_model)))
tr <- df_model[idx.tr,]
ts <- df_model[-idx.tr,]
ts = rbind(tr[1,],ts)
ts = ts[-1,]
require('randomForest')
#df_model1 <- df_model[, -c('LOCAL_AUTHORITY_DISTRICT')]
# Make several estimations and select the best model
res_RF <- performanceEstimation(
PredTask(NUMBER_OF_CASUALTIES ~ .,tr),
workflowVariants(learner="randomForest", learner.pars=list(ntree= c(100, 500, 1000))),
EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))
# Save the model results
save(res_RF, file = "res_RF.RData")
# Load the data
load(file = "res_RF.RData")
# Plot the graphs
plot(res_RF)
# See the performance rank
topPerformers(res_RF)
# Make several estimations and select the best model
res_SVM <- performanceEstimation(
PredTask(NNUMBER_OF_CASUALTIES ~ .,tr),
workflowVariants(learner="svm", learner.pars=list(kernel=c("linear", "radial"), cost=c(1,3), gamma=c(0.1,0.01), epsilon=0.1)),
EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))
# Save the model results
save(res_SVM, file = "res_SVM.RData")
res_SVM <- performanceEstimation(
PredTask(NUMBER_OF_CASUALTIES ~ .,tr),
workflowVariants(learner="svm", learner.pars=list(kernel=c("linear", "radial"), cost=c(1,3), gamma=c(0.1,0.01), epsilon=0.1)),
EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))
# Save the model results
save(res_SVM, file = "res_SVM.RData")
# Load the data
load(file = "res_SVM.RData")
# Plot the graphs
plot(res_SVM)
# See the performance rank
topPerformers(res_SVM)
# Make several estimations and select the best model
res_EXP <- performanceEstimation(
PredTask(NUMBER_OF_CASUALTIES ~ ., tr),
c( Workflow(learner="naiveBayes"),
workflowVariants(learner="svm", learner.pars=list(kernel=c("linear", "radial"), cost=c(1,3))),
Workflow(learner="randomForest", learner.pars=list( ntree=3000))
),
EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))
# Save the model results
save(res_EXP, file = "res_EXP.RData")
# Load the data
load(file = "res_EXP.RData")
# Plot the graphs
plot(res_EXP)
# See the performance rank
topPerformers(res_EXP)
# Make several estimations and select the best model
res_TREES <- performanceEstimation(
PredTask(NUMBER_OF_CASUALTIES~ .,tr),
workflowVariants(learner="rpartXse",learner.pars=list(se=c(0,0.5,1, 2, 3))),
EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))
# Save the model results
save(res_TREES, file = "res_TREES1.RData")
# Load the data
load(file = "res_TREES1.RData")
# Plot the graphs
plot(res_TREES1)
# See the performance rank
topPerformers(res_TREES1)
# Get the parameters
(getWorkflow("rpartXse.v2",res_TREES1))
plot(res_TREES1)
load(file = "res_TREES1.RData")
# Plot the graphs
plot(res_TREES)
# See the performance rank
topPerformers(res_TREES)
# Get the parameters
(getWorkflow("rpartXse.v2",res_TREES))
# Load the data
load(file = "res_RF.RData")
# Plot the graphs
plot(res_RF)
# See the performance rank
topPerformers(res_RF)
# Load the data
load(file = "res_SVM.RData")
# Plot the graphs
plot(res_SVM)
# See the performance rank
topPerformers(res_SVM)
# Load the data
load(file = "res_EXP.RData")
# Plot the graphs
plot(res_EXP)
# See the performance rank
topPerformers(res_EXP)
# Load the data
load(file = "res_MODELS.RData")
# Plot the graphs
plot(res_MODELS)
# See the performance rank
topPerformers(res_MODELS)
save(res_MODELS, file = "res_MODELS.RData")
res_MODELS <- performanceEstimation(
PredTask(NUMBER_OF_CASUALTIES ~ ., tr),
workflowVariants("standardWF",
learner=c("rpartXse","svm","randomForest")),
EstimationTask(metrics="mse",method=Holdout(nReps=3,hldSz=0.3)))
# Save the model results
save(res_MODELS, file = "res_MODELS.RData")
# Load the data
load(file = "res_MODELS.RData")
# Plot the graphs
plot(res_MODELS)
# See the performance rank
topPerformers(res_MODELS)
